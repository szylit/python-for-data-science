{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python for Data Science Teaching Session 4: Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Combining datasets and filtering joins\n",
    "- Dealing with missing values\n",
    "- Reshaping data\n",
    "- Dummifying categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coming Full Circle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session, we will be returning to Pandas, to learn how we can use it for transforming data. We will start by importing the package as well as another we'll need for loading the datasets we'll be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will be working with a collection of datasets related to airline data for planes departing NYC in 2013. The code below loads in a selection of dataframes from this course's GitHub repository, saving them in a dictionary, `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = ('https://raw.githubusercontent.com/warwickdatasciencesociety/'\n",
    "            'python-for-data-science/master/session-four/data/nycflights13/')\n",
    "# Collect list of file names\n",
    "with urlopen(base_url + 'names.txt') as f:\n",
    "    names = f.read().decode('utf-8').split()\n",
    "    \n",
    "# Import data\n",
    "data = {}\n",
    "for n in names:\n",
    "    data[n] = pd.read_csv(base_url + n + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a list of the dataframes and view any particular one using standard dictionary techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset list\n",
    "print(*data.keys(), sep=' | ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Airlines dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin by learning how to concatenate data. Concatenation is used when we have two dataframes that either share the same columns or same rows. We can then join along the non-matching axis to obtain a single dataframe. This is done using the `pd.concat()` function which accepts a list-like object of dataframes to combine as well as the axis to combine over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by concatenating rows. In this case, we use `axis=0`. This is the default, so we don't need to specify it, but it is helpful to people reading our code later (which may even be us) to include it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this for the two flights dataset, which contain the same columns but for different quarters. Before combining, it is best practice to check that the columns do indeed match up (though order doesn't matter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check both flight datasets have the same columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the flights datasets\n",
    "data['flights'] = # ...\n",
    "data['flights'].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note, there is no limit to how many dataframes you can pass into `pd.concat`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, we use `axis=1` to be combine columns. We can use this to combine the two datasets giving location and timezone information for airports. In this case, both datasets contain the columns `faa` and `name` so we will remove these from the latter. Like before, we check the indices match before concatenating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check both airport datasets have the same rows\n",
    "set(data['airports_loc'].index) == set(data['airports_tz'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the airports datasets\n",
    "data['airports'] = # ...\n",
    "data['airports'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few edge-cases we might want to consider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we don't have matching rows/columns? In this case, the default behaviour is to use missing values to fill any gaps. If however, we specify `join='inner'`, any rows/columns not found in all dataframes will be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we have duplicate columns or indexes? This can result in strange behaviour as this is not the intended use case of `concat`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you find yourself in either of the two situations above, you probably want a join, which we will learn about now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Further Reading**\n",
    ">\n",
    "> The `pd.concat()` function has a large number of optional parameters for handling special cases and applying post-processing steps. Read [the documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html) to learn more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merges and Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many programming languages and database management systems facilitate the joining of data. This is used when we have two columns in two dataframes that represent the same variable and we intend to use this to combine the data into one table. This differs from concatenation in that there is no expectation that the two columns columns used in the join share the same values or that there are no duplicates. In fact, joins are meant for exactly these two cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create two dummy datasets to introduce the concept of joins. These relate to the employees in a fictional company and the sales they made. Notice how Cat made no sales, and sales were made by Dan despite him no longer being an employee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create example datasets\n",
    "employee = pd.DataFrame({\n",
    "    'name': [\"Ann\", \"Bob\", \"Cat\"],\n",
    "    'years_exp': [4, 2, 1]\n",
    "})\n",
    "sale = pd.DataFrame({\n",
    "    'name': [\"Ann\", \"Bob\", \"Dan\", \"Ann\", \"Dan\", \"Bob\", \"Ann\"],\n",
    "    'day': [\"Mon\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Thu\", \"Fri\"],\n",
    "    'value': [5, 7, 3, 8, 3, 4, 8]\n",
    "})\n",
    "display(employee)\n",
    "display(sale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column used to join the two dataframes is referred to as the key, in this case it is the employee name. There are four common types of join that differ in how they handle values that don't appear in one of the key columns:\n",
    "\n",
    "- `inner` (default): keep only rows that have matching key values in both dataframes\n",
    "- `left`: keep only rows for every key value in the left-hand dataframe\n",
    "- `right`: keep only rows for every key value in the right-hand dataframe\n",
    "- `outer`: keep all rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All joins but the inner join can introduce missing values, as we will see in a moment. When we talk about the left/right-hand tables, we are referring to which comes first (left) and second (right) in the function call. Confusingly, Pandas uses the function `merge` to perform what is typically called a `join`. The `join` function performs a similar function which we will see later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest join is the inner join, only keeping key values that appeared in both tables. We perform an inner join as so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform an inner join\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A left join, gives precedence to the left-hand dataframe, keeping all values of its key column. This might involve filling unknown values with `np.nan`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a left join\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A right join has the reverse effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a right join\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An outer join (also called a full/full outer join) is the most lenient, including any key value that appears in either table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform an outer join\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session, we don't have to time to explain why we store data in a format that requires joining or why we would choose one join type over another. We will instead discuss this in the project sessions with a practical example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case when our key columns have different names, we use `left_on` and `right_on` instead of the single `on`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A join in Pandas is used when we want to join the index of one dataframe with a column or index from another. Essentially, it is a more restrictive version of `merge` (which can also use indices—check [the docs](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html)), enjoying more efficiency in return. Since `merge` encompasses this functionality, we will look at one example and then move on. Feel free to read up more on `join` if you are keen to use best practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert name column to an index\n",
    "employee_index = employee.set_index('name')\n",
    "employee_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a join using the index\n",
    "sale.join(employee_index, on='name')  # default is 'left'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Going Beyond**\n",
    ">\n",
    "> We've only just scratched the surface of what Pandas joins and merges can do. Here are a few extra ideas for you to play with:\n",
    ">\n",
    "> - We can join by a key composed of multiple columns by using a list of column names for `on`, `left_on`, `right_on`\n",
    "> - We can also write `pd.merge(df1, df2, ...)` as `df1.merge(df2, ...)`\n",
    "> - There is another type of join called a cross-join used for creating all combinations of keys\n",
    "> - When dataframes have overlapping column names, suffixes are applied to each, which can be specified using the `suffixes` parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Trouble About**\n",
    ">\n",
    "> You may have noticed that the above solution breaks down if we have two employees with the same name. In this case, we need unique identifiers (UIDs). More on this in WDSS's deep Dive into SQL course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The joins above are known as mutating joins, as they mutate the structure of the dataframes. Another type of join is the filtering join. This is where we filter one dataframe using a key column based on the presence of values in another dataframe's corresponding key column. \n",
    "\n",
    "Whereas languages such as R have dedicated functions for this, Pandas is optimised enough to use `.isin()` and `.unique()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An semi-join is where we only include rows with key values that can be found in the other dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a semi-join\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An anti-join is the reverse, keeping only rows that _don't_ match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform an anti-join\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will have noticed above that join operations can introduce missing values, denoted as `NaN`. These also commonly arise in real-world datasets, either because we are missing data or because data was invalid. It is important to be able to find missing values and handle them appropriately. To learn more about missing values, we will work with the `adult` dataset, which contains information about various US adults and their income level. You can download this dataset from the [session webpage](https://education.wdss.io/python-for-data-science/session-four/). The dataset contains many missing values which are marked with a \"?\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names not included in dataset so we add them manually\n",
    "column_names = (\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education-num', \n",
    "    'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "    'capital-gain', 'capital-loss', 'hours-per-week', 'native-country',\n",
    "    'income'\n",
    ")\n",
    "adult = pd.read_csv('data/adult.csv', names=column_names, na_values='?')\n",
    "# Drop unnecessary columns\n",
    "adult = adult.drop(['fnlwgt', 'education'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding and Removing Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we ask how we can find missing values. We do this using the `isna()` method. This returns a Boolean dataframe the same size as the original, with `True` entries wherever the original data was missing. We can then use our usual aggregations to ask questions about the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What proportion of each column is missing?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which row has the most missing values? Print it out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most aggressive method of handling missing values is to either drop columns or rows with missing values. This can be done using the `dropna` method, specifying the axis to specify if we wish to remove rows (`0`, the default) or columns (`1`). By default, rows/columns with any missing values are removed but by modifying the parameters this can be changed to all missing or a above a certain number. Read more in [the docs](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all rows with missing values from `adult` and report the change in size\n",
    "print(\"Number of rows before:\", adult.shape[0])\n",
    "adult_dropna = # ...\n",
    "print(\"Number of rows before:\", adult_dropna.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, we know what value a missing value represents. For example, when we performed a left join before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employeed_sales = pd.merge(employee, sale, how='left', on='name')\n",
    "employeed_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, absence from the sales table implies that no sales were made and so we can replace missing values in the `value` column with `0`. We do this using the `fillna()` method. This can be applied to the entire dataframe or just one series. The method can do complex fills such as back-filling and padding, but we will look at the simplest case when we pass in a single number to fill missing values with. You can read about the other options [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much did each employee make in sales?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two methods of handling missing values above are rather heavy handed. Instead, we may wish to use a predictive approach. This is known as imputation, and is where we guess sensible values of the missing fields. In advanced cases, this may involve using the known features to predict the missing ones, but we'll focus on a simpler case in which we fill missing values based on the non-missing values in the same column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When dealing with missing values in continuous columns it makes the some sense to fill these values with the column mean or median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing ages with the median age\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For discrete variables, these may give nonsensical results, so it is often better to use the column mode (most common value). We calculate this using the `value_counts()` method before extracting the first index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing countries with modal country\n",
    "modal_country = # ...\n",
    "print(\"Modal country:\", modal_country)\n",
    "adult['native-country'] = # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For a deeper discussion on how to handle missing values, check out [episode 5](https://youtu.be/BIoFwGl2Vtc?t=1150) of WDSS's podcast [DataBasic](https://podcast.wdss.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often the case that we have the correct data but in the wrong shape. In this section we will learn how to reshape data from one form to another. This is an especially important skill if you often work with data stored in Excel, as the free-form nature of spreadsheets can often encourage peculiar data structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not have the time in this course to go into detail about the pros and cons of various dataframe shapes. Instead, I strongly recommend you watch 4:25–36:42 of [session four](https://youtu.be/_1vSZ1NMgNI?t=265) from WDSS's 'Into the Tidyverse' course. The teaching is in R but the ideas presented about 'tidy' data are language agnostic and of much importance. Pandas does not take the principles of tidy data as strictly as in R's tidyverse, but they are still crucial to be aware of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest reshaping method is transposition. This involves swapping the columns and rows of our dataset, flipping the entire table along its diagonal. We do this using the `.transpose()` method (which copies by reference). We will create a mock dataframe below in which it would be sensible to perform a transposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock data (almost certainly) requiring transposition\n",
    "weather = pd.DataFrame({\n",
    "    2016: [16, 1, 0.24],\n",
    "    2017: [15, 4, 0.21],\n",
    "    2018: [13, 0, 0.25],\n",
    "    2019: [26, 2, 0.32],\n",
    "}, index=['rainfall', 'snowfall', 'cloud_coverage'])\n",
    "weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose the weather dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Word of Warning**\n",
    ">\n",
    "> Watch out for changing column types when reshaping data. Columns in Pandas must be of one type and so when we switch rows/columns or perform the other transformations that we'll see in a moment, we can have unexpected type coercion, which we may need to correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivoting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pivoting is an operation in Pandas that we perform when we have a column containing variable names or indexes values. We perform a pivot using the `pivot` method, passing in the columns in our current dataframe whose values should be turned into indexes, columns, or stay as values. We'll again work on a mock example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock data (likely) requiring pivoting\n",
    "weather = pd.DataFrame({\n",
    "    'year': [2016, 2016, 2017, 2017, 2018, 2018],\n",
    "    'variable': ['rainfall', 'snowfall', 'rainfall',\n",
    "                 'snowfall', 'rainfall', 'snowfall'],\n",
    "    'value': [16, 1, 15, 4, 13, 0]\n",
    "})\n",
    "weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Good to Know**\n",
    ">\n",
    "> We can also pass lists of column names as any arguments of `pivot`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Why Can't We All Agree**\n",
    ">\n",
    "> Don't confuse Pandas pivoting with:\n",
    ">\n",
    "> 1. Excel's pivot tables which are performed using `pd.pivot_table`\n",
    "> 2. R's pivoting, which includes both pivot (`pivot_wider`/`spread`) and melting (`pivot_longer`/`gather`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Melting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reverse operation of pivoting is melting. This takes multiple columns and turns them into a pair of columns, one containing variable names, the other values. We do this using the `melt` method, passing in a list of variables to be kept the same `id_vars` and (optionally) the name of the new column to create `var_name`. We again practice this on a mock dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock data (possibly) requiring melting\n",
    "measurements = pd.DataFrame({\n",
    "    'name': [\"Ann\", \"Bob\", \"Cat\"],\n",
    "    'height': [157, 172, 168],\n",
    "    'weight': [10.3, 12.4, 11.2],\n",
    "    'pace': [6.2, 5.9, 5.4]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt the three measurement columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **To Reshape or Not To Reshape**\n",
    ">\n",
    "> It can sometimes be hard to know when to reshape data and towards what target. It is first worth noting that the best shape for data is highly contextual, depending on our objectives and whether we are focusing on human or computer readability. In general it is best to follow tidy data practices; as with all rules, however, the sign of an experienced Pandas user is that they know when to break them. This is something you will learn with time and through looking at others' code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important data reshaping technique is stacking and unstacking. This is right on the border between intermediate and advanced techniques, so I have decided to cover it at the start of my office hours this week (a recording of which will be made available), alongside a brief discussion about multi-indexes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computers like numbers. So far we have seen how powerful NumPy is and we will likewise see the same with regard to scikit-learn in session five. These tools however rely on having a single datatype for our data which is most often chosen to be numeric. For that reason, we need to find a way of converting our text variables to numeric forms. This process is known as _encoding_ the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common encoding technique for **categorical** text variables is called one-hot encoding. This involves creating a new binary column for every possible value the categorical variable takes, taking value one if and only if the original value corresponds to that column. This is best seen with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![One-hot encoding example](images/one_hot_enc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These new columns of zero and ones are known (particularly in economics) as dummy variables. We can generate these using `pd.get_dummies`, which accepts a series or dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummify the race column of `adult`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our dummies, we can drop the original column and concatenate our dummies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hang on a second though. Let's have a look at the unique values of `adult.race`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult.race.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's subtle, but there is a possible problem here. Namely, we have redundant information in our model. Every observation has a corresponding race and so if all but one of the dummy columns are zero, the last must be a one. Therefore, we don't actually need all columns but can instead one. In fact, for many algorithms (such as linear regression), this will prevent us from fitting a model. We can avoid this by using `drop_first=True` or by manually dropping a column. In this case, it makes sense for us to drop `Other`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummify the race column of `adult` and drop `Other`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Further Reading**\n",
    ">\n",
    "> Although one-hot encoding is the most popular encoding method, there are many others available which shine in particular use cases. One other prominent example is ordinal encoding. Learn more about these [here](https://www.analyticsvidhya.com/blog/2020/08/types-of-categorical-data-encoding/), being aware that this includes techniques from `sklearn` which we will learn about in session five."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding and similar methods work fine for categorical data with only a few levels, but what about when we have a large or virtually infinite number of levels. One approach would be to group various categories together but this is only a temporary solution. A far more powerful solution is to use learned embeddings. An embedding is a way of mapping an input to numeric data space that attempts to capture the structure and relations of the original inputs. These are often learnt using advanced machine learning techniques such as neural networks.\n",
    "\n",
    "Thankfully, for many cases, trained models are available to us. One example is Word2Vec, which takes English words and maps them to a data space that encodes semantic meaning. This means that similar words are close together and we can form analogies such as `Queen - Woman + Man = King`. Read more about Word2Vec in this [WDSS blog post](https://research.wdss.io/word2vec/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date and Times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this course, we've paid little specific focus to handling dates and times with Pandas. Unfortunately, we not have time to cover this. It is however vital to be aware of and so we hope to run workshops on the topic eventually. For now, you can read more about the available functionality in [this article](https://pandas.pydata.org/docs/user_guide/timeseries.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Window functions are an advanced feature of Pandas that allow you to \"roll\" functions over a dataframe. In most cases they are not needed or can be avoid by using loops, but for big data cases, the efficiency they provide is essential. Read more about them [here](https://pandas.pydata.org/docs/user_guide/window.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Styling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas dataframes can look a bit dull. Thankfully, there is a flexible API available for styling dataframes, including highlighting specific observations, columns, and fields. You can even colour-code fields as you would with conditional formatting in Excel. The full details can be found [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/style.html), but here is an example to whet your appetite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult.iloc[:5, [0, 2, 8]].style.background_gradient(cmap='viridis')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python for Data Science",
   "language": "python",
   "name": "pyds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
