{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "391b186c",
   "metadata": {},
   "source": [
    "# Python for Data Science Practice Session 4 : Mathematics and Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86afe2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import pandas, numpy, numpy.random and matplotlib.pyplot \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf34d50",
   "metadata": {},
   "source": [
    "## Missing data types: A closer look"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6e751a",
   "metadata": {},
   "source": [
    "In this section, I would like to talk more about missing values. It is one of the most influential problems that have to be dealt with carefully by Data Scientists, and even by other practitioners in different fields who analyse and use data to help in forming decisions/conclusions. Together, we will go through the three main types of missing values: Missing Completely at Random, Missing at Random and Missing Not at Random. Then, we will build up on the imputation methods discussed in this week's teaching session by using two more complex ones, namely Expectation Maximization and Linear Regression. We will also make use of the dataframe reshaping methods that we learned in this week's teaching session to help make our lives easier throughout this project session."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f572c77",
   "metadata": {},
   "source": [
    "Let's start by generating a mock dataset that we will use to demonstrate the different missing value types. This dataset will contain simulated data of three consecutive lap times for ten F1 racing drivers on a track.\n",
    "\n",
    "Start by creating a Pandas dataframe with column labels 'D1', 'D2', 'D3', ... , 'D10' representing the ten drivers we have, and indexes 'Lap1', 'Lap2' representing the first two lap times for each driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "093e75ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the dataframe as specified above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbb0e23",
   "metadata": {},
   "source": [
    "Now, we simulate lap time values from normal distributions. Assume that for the first lap times, data are normally distributed with a mean of 150 and standard deviation of 10. Then in lap 2, data are normally distributed with a mean of 145 and a standard deviation of 10. Simulating the data in this way allows for the possibility for some drivers improving their lap time in their second trial and some other drivers worsening their lap time in their second trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4285ef3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulate data from normal distributions for laps 1 and 2 using the means and standard deviations specified above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f15b32a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c2feb6",
   "metadata": {},
   "source": [
    "The structure of the dataframe is not visually pleasing in my opinion, especially that we have ten columns and only two rows. Try transposing the dataframe: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efe5bf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transpose the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b65f6257",
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0d9a7d",
   "metadata": {},
   "source": [
    "This looks much better, let us now proceed. \n",
    "\n",
    "Now, create a column for lap 3 times where drivers who score better lap times in their second lap than their first lap have their data generated from a normal distribution of mean of 140 and standard deviation of 5, and drivers who score worse in their second lap than their first lap have their lap 3 times generated from a normal distribution of mean 155 and standard deviation of 5. This can be thought of in the following sense: The drivers who do better in lap 2 than lap 1 are more likely to be motivated and improve on their lap 2 times with a faster lap 3 time, and people who score worse in lap 2 might be disappointed and thus more likely to score worse in lap 3. (Of course you can argue that this is not usually the case in real life, but for simplicity, we will proceed with it as it will help us in the explanation of the different missing data types)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99253131",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the column Lap 3 following the scenario mentioned above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a15f8ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b4f8f7",
   "metadata": {},
   "source": [
    "## Missing Completely At Random (MCAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1665c9",
   "metadata": {},
   "source": [
    "From its name, missing data are said to be missing completely at random if they are missing due to completely random reasons. There is no specific pattern in the dataset: \n",
    "- No relationships between the variable with missing values and other variables.\n",
    "- No relationship between the variable with missing values and itself. \n",
    "\n",
    "An example for this in our dataset would be:\n",
    "- The tracking device on one of the cars suddenly got damaged.\n",
    "- A cars' engine broke down before finishing one of the laps.\n",
    "\n",
    "We are now going to create a column called MCAR which could represent Lap 3 times from real life data where the missing Lap 3 times for some of the drivers are missing completely at random. \n",
    "\n",
    "There are plenty of ways to do so, but the one I would hint towards is multiplying each lap 3 times with a number simulated from a binomial distribution with probability of success equalling 0.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a240ed1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the column called MCAR for Lap 3 times, with some of them missing completely at random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e940ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a84fd17",
   "metadata": {},
   "source": [
    "## Missing At Random (MAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50265249",
   "metadata": {},
   "source": [
    "Missing values are said to be Missing at Random if they are missing due to a dependance on other variable/s. This means that for any observation, a variable being missing depends on other variable/s' values. An example for this in our dataset would be: \n",
    "\n",
    "- Drivers who scored 10-seconds slower lap 2 times or more than lap 1 times are ordered by their teams not to attempt a third lap as this might indicate loss of power in the car's engine. So, the availability of lap 3 times are affected by the times for laps 1 and 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72e48a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a column called MAR that follows the scenario in the above bulletpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40862525",
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4017f10",
   "metadata": {},
   "source": [
    "## Missing Not at Random (MNAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25782e2c",
   "metadata": {},
   "source": [
    "Missing values are said to be Missing Not at Random if they are missing because of their values themselves. This might sound unintuitive, but here is an example from our dataset that will make it clearer: \n",
    "\n",
    "- Assume that drivers on this track generally score in the range of 120-160 seconds. Drivers who score above 160 seconds on Lap 3 get disheartened by that and sometimes decide not to report their scores as they are afraid it will reflect badly on their performance. This means that missing values in lap 3 are missing because of the time scored in lap 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4ddb918",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a column called MNAR that follows the scenario in the above bulletpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71570642",
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a3bf41",
   "metadata": {},
   "source": [
    "A good question right now would be: How to detect whether the missing values in your dataset are MCAR, MAR or MNAR? \n",
    "\n",
    "Here is a good discussion I found on Kaggle: https://www.kaggle.com/questions-and-answers/105010 \n",
    "\n",
    "Now, we move onto the more advanced missing value imputation methods, starting with Expectation Maximization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b199dd6",
   "metadata": {},
   "source": [
    "## Expectation Maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd24cdb",
   "metadata": {},
   "source": [
    "Expectation Maximization is an iterative algorithm used to calculate the maximum likelihood estimates for the parameters of a statistical model that depends on latent variables (or unobserved variables). Due to its nature of working with statistical models of unobserved variables, it is suitable for missing data imputations.\n",
    "\n",
    "In more details, suppose you have an explicit form for the joint distribution of $X_{obs}$ (the observed data), and $X_{mis}$ (the missing data). The goal is to estimate the parameters $\\theta \\in \\mathbb{R}^{d}$ for the statistical model of the joint distribution ($X_{obs}$,$X_{mis}$) by calculating the maximum likelihood estimates for the likelihood function of the marginal distribution of $X_{obs}$. We have that:\n",
    "$$\n",
    "L(\\theta ; X_{obs}) = p(X_{obs} | \\theta) = \\int p(X_{obs},X_{mis} | \\theta) \\, dX_{mis} = \\int p(X_{obs} | X_{mis},\\theta) p(X_{mis}|\\theta) \\, dX_{mis} \n",
    "$$\n",
    "As we do not observe the missing values $X_{mis}$, we cannot always compute the above explicitly. \n",
    "\n",
    "This is where EM comes into play. It obtains the maximum likelihood estimates for the marginal distribution of $X_{obs}$ by iteratively maximizing the expected complete-data log likelihood. Here are the two steps of the iteration:\n",
    "\n",
    "Start with an initial estimate $\\theta^{(0)}$ and let $\\theta^{(t)}$ be the iterate for the parameter $\\theta$ in the  $t^{th}$ iteration of the algorithm.\n",
    "\n",
    "<b> 1) Expectation: </b> Compute the expectation of the log likelihood of the complete-data with respect to the conditional distribution of $X_{mis}$ parameterized by $\\theta^{(t)}$: \n",
    "$$\n",
    "Q(\\theta, \\theta^{(t)}) = \\mathbb{E}_{X_{mis} | X_{obs},\\theta^{(t)}}[log(l(\\theta ; X_{obs}, X_{mis})]\n",
    "$$\n",
    "\n",
    "<b> 2) Maximization: </b> Obtain the value of $\\theta$ that maximizes Q($\\theta$, $\\theta^{(t)})$:\n",
    "\n",
    "$$\n",
    "\\theta^{(t+1)} = argmax_{\\theta} \\, Q(\\theta, \\theta^{(t)})\n",
    "$$\n",
    "\n",
    "The algorithm keeps iterating until it reaches a point where the difference between the estimates is negligible, and the algorithm is thought to have converged.\n",
    "\n",
    "In a nutshell, the algorithm starts with $\\theta^{(0)}$, then it estimates the values for the missing data using the observed data and the parameters $\\theta^{(0)}$, then it calculates the maximum likelihood parameter estimates for the complete-data, then it estimates new values for the previous missing data, and then the process keeps repeating until it converges. \n",
    "\n",
    "\n",
    "Expectation Maximization only works with MCAR and MAR missing data, and it works specifically well with distributions from the exponential family. One thing to bear in mind is that it calculates the local maximum likelihood estimates, and so for multimodal distributions, the global likelihood might not be obtained. \n",
    "\n",
    "- - - - - -\n",
    "\n",
    "Let us work through a simulated dataset from a multivariate normal distribution, where we are going to remove data randomly using the help of a binomial distribution as we did before; so that they fall under the MCAR missing data category.\n",
    "\n",
    "We are going to simulate random vectors of size 3 from a multivariate normal distribution with the following mean vector and variance covariance matrix (Feel free to change the parameters if you'd like):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32012fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The mean vector\n",
    "Mean = random.normal(10,5,3)\n",
    "Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1dccf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The variance-covariance matrix\n",
    "Cov_matrix = np.array([[1,1,1], [1,1,1], [1,1,1]])\n",
    "Cov_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70631dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate 200 vectors from a multivariate normal distribution with the mean vector and var-cov matrix specified above \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364ecd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the first 10 vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dc4d38",
   "metadata": {},
   "source": [
    "Now, we are going to remove some of the data randomly from the third entry of the vectors so that we could then try imputing them using EM and other simple imputations methods. \n",
    "\n",
    "Let us start by creating a copy from the array so that we could have a full array and another array with MCAR values in the third entry that we could impute and compare to the original array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88b7d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a copy from the original array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9df334",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove data from the third entry randomly (as done before)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6d6ec3",
   "metadata": {},
   "source": [
    "Download a library called `impyute` using `pip install impyute`. \n",
    "\n",
    "Then, import impyute as impy into the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627d22c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import impyute\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b10107",
   "metadata": {},
   "source": [
    "Quickly research how impyute is used, and then create three arrays where each array has its missing values imputed using one of the following methods: \n",
    "\n",
    "- Expectation Maximization\n",
    "- Mean\n",
    "- Median "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d15f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the array imputed with EM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba9015d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the array imputed with the mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fc82ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the array imputed with the median\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e30b88",
   "metadata": {},
   "source": [
    "## Comparing them together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2de3b65",
   "metadata": {},
   "source": [
    "Now, we are going to specify the entries that were imputed in the three imputation methods, and then compare them by concatenating them together alongside the original data, and then using matplotlib to plot a graph for the data from the three imputed methods as well as the original data. \n",
    "\n",
    "Let us start by specifying the indices of the observations where there was a missing value in the third entry: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74bf7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an array with the indices of the observations where there was a missing value in the third entry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2af260e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an array with the EM imputed values \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d115340d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an array with the mean imputed values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fc83d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an array with the median imputed values \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d569347",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an array with the original data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0832452",
   "metadata": {},
   "source": [
    "Now, create three datasets where each dataset contain the imputed values from one data imputation method, and one final dataset with the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd199b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataset with the EM imputed values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78c05dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataset with the mean imputed values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07ea151",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataset with the median imputed values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a7f5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataset with the original values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b48b94",
   "metadata": {},
   "source": [
    "Now, concatenate the four datasets together to help in the comparison between the values from the different imputation methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a251c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate the above four datasets \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbebd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the new dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4125c2",
   "metadata": {},
   "source": [
    "Now, plot a line graph for the data imputed from the three imputation methods as well as the original data in one plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f4f57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot a line graph for the data imputed from the three imputation methods and the original data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274aab62",
   "metadata": {},
   "source": [
    "- - - - - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7b7b09",
   "metadata": {},
   "source": [
    "## Linear Regression imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a08a3f",
   "metadata": {},
   "source": [
    "If you have a dataset where one of the variables has some missing values, and this variable correlates strongly with one other or more variables in the dataset, then Linear Regression imputation might work really well. \n",
    "\n",
    "It fits a linear regression model where the variable with the missing values is the target/response variable, and the other variable/s (has no missing data) that correlate with it as the predictor variable/s. \n",
    "\n",
    "Because relationships/correlations between variables are conserved in this imputation method, it has an advantage over the simpler imputation methods such as the mean and median in some of the cases.\n",
    "\n",
    "There are two versions for this imputation method: \n",
    "\n",
    "<b> 1) Deterministic Regression Imputation: </b>\n",
    "\n",
    "This is when the missing values are imputed with the exact predictions from the linear regression model, without adding any error terms to it. A disadvantage for this is that it reduces the variability of the imputed variable. This is because the imputed values would lie exactly on the regression hyperplane, which is not a very good representative of real world data. This leads to the second version.\n",
    "\n",
    "<b> 2) Stochastic Regression Imputation: </b>\n",
    "\n",
    "This follows the same idea of Deterministic Regression Imputation but with the addition of a random error term to the predicted value. \n",
    "\n",
    "You will try by yourself an example of imputing missing data using both methods to see how each one performs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc37d342",
   "metadata": {},
   "source": [
    "Again, let us simulate two variables that are correlated.\n",
    "\n",
    "Start by generating 100 data points from a normal distribution of mean 5 and standard deviation 1. Call this array col1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b275121e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 100 data points from a normal distribution of mean 5 and standard deviation 1. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0f4a5a",
   "metadata": {},
   "source": [
    "Simulate another random variable that correlates with it. Remember to add a random noise/term to prevent the two variables from correlating perfectly. Call this array col2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991eafab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate another 100 data points that correlate with the previously generated 100 data points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198177ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a scatterplot for col2 against col1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da08beae",
   "metadata": {},
   "source": [
    "Again, we will remove data randomly from col2 using the binomial distribution; so that the missing data fall under the MCAR type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a609735",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a copy of v2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eac73d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove data randomly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615b8e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#View v2 with missing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918ae423",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the indices of the observations that having a missing value for v2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f16a0c3",
   "metadata": {},
   "source": [
    "Now, we are going to fit a linear regression model for the observations with no missing data. Recall in the third project session, we went through the steps of creating a linear regression model step by step. Now, we are going to make use of a library called `sklearn` that has a built-in function called LinearRegression. \n",
    "\n",
    "Read more about how to use it here, and then use it to fit a linear regression model where v1 is the predictor variable and v2 is the target/response variable. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e7039a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import LinearRegression from sklearn.linear_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e919c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the indices of the observations that have no missing values \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da338e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create X_train for the v1 values for observations with no missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aac020c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create y_train for the v2 values for observations with no missing values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08fba8f",
   "metadata": {},
   "source": [
    "The LinearRegression function requires the X_train values to follow a specific format, where each entry is an array on its own, and all of those individual arrays are stacked on top of each others. I will reformat it for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "2968c25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.reshape(X_train, [X_train.size,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ed03d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the linear regression model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408b1883",
   "metadata": {},
   "source": [
    "## Deterministic Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c22bd9",
   "metadata": {},
   "source": [
    "Let us start by imputing the missing values using the Deterministic Linear Regression approach. \n",
    "\n",
    "Start by using the fitted regression model to predict values for the missing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85c0c463",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an array of the v1 values of the observations that have missing values \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fdac7cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping of the array to be suitable for usage in the regression model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f97559f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an array for the missing v2 values \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324f8410",
   "metadata": {},
   "source": [
    "Now, create a dataset where each row has the imputed value of v2 by linear regression, and the original value of v2 from the original array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79fa0730",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the dataset as specified above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "118b3fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7acc9e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the observations using the original values and the observations with the imputed values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53e3dd1",
   "metadata": {},
   "source": [
    "Now, I will leave you to try and impute data using Stochastic Linear Regression. This resource might help you out: https://www.kaggle.com/shashankasubrahmanya/missing-data-imputation-using-regression but feel free to do your own research."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
